{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word2vec on Twitter data\n",
    "\n",
    "Limitations in obtained results: \n",
    "- low cosine similarity between words which are supposed to be close (\"prix\" & \"inflation\")\n",
    "- high computational time to retrieve embeddings for all tweets. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl \n",
    "from itertools import chain\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Union\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../backup/data/tweets_preprocessed.pkl\"\n",
    "with open(path, \"rb\") as f: \n",
    "    tweets_preprocessed = pkl.load(f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create corpus "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Corpus: \n",
    "    \"\"\"Description. An iterator that yields sentences (lists of str).\n",
    "    \n",
    "    Attributes: \n",
    "        - sentences: list of tokens or plain texts\n",
    "        - tokenized: indicates whether the text has been tokenized\"\"\"\n",
    "\n",
    "    def __init__(self, sentences: List[Union[List, str]], tokenized: bool):\n",
    "        self.sentences = sentences\n",
    "        self.tokenized = tokenized\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\"Description. Iterator over sentences.\"\"\"\n",
    "\n",
    "        for s in self.sentences:\n",
    "            if self.tokenized:\n",
    "                s = \" \".join(s)\n",
    "            yield simple_preprocess(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = Corpus(sentences=tweets_preprocessed[\"cleaned\"], tokenized=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_words = set(list(chain(*corpus))) \n",
    "n_words = len(unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['le', 'tout', 'vendu', 'des', 'prix', 'défiant', 'toute', 'concurrence', 'ceci', 'pour', 'nous', 'annoncer', 'prochainement', 'que', 'ayant', 'un', 'trou', 'dans', 'notre', 'économie', 'circulaire', 'nous', 'allons', 'participer', 'au', 'renflouement']\n"
     ]
    }
   ],
   "source": [
    "for text in corpus: \n",
    "    break \n",
    "\n",
    "print(text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = 200\n",
    "window = 5\n",
    "min_count = 5\n",
    "\n",
    "model = Word2Vec(\n",
    "    sentences=corpus, \n",
    "    min_count=min_count,\n",
    "    vector_size=n_components, \n",
    "    window=window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = model.wv.index_to_key"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = model.wv.index_to_key\n",
    "vocab_len = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "w2v vocab: ['de', 'la', 'le', 'les', 'et', 'des', 'en', 'inflation', 'est', 'pour']...\n",
      "number of words in vocab=29850\n",
      "number of words in total=113833\n"
     ]
    }
   ],
   "source": [
    "print(vocab_len == len(set(vocab)))\n",
    "\n",
    "print(f\"w2v vocab: {vocab[:10]}...\")\n",
    "print(f\"number of words in vocab={vocab_len}\")\n",
    "print(f\"number of words in total={n_words}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tarifs', 0.5407853722572327),\n",
       " ('litre', 0.5063979625701904),\n",
       " ('tarif', 0.48051708936691284),\n",
       " ('loyers', 0.4737620949745178),\n",
       " ('électricité', 0.4498542249202728),\n",
       " ('coût', 0.4493483603000641),\n",
       " ('coûts', 0.44890135526657104),\n",
       " ('pâtes', 0.4442346692085266),\n",
       " ('gaz', 0.4326724708080292),\n",
       " ('panier', 0.4324531555175781),\n",
       " ('pain', 0.41085925698280334),\n",
       " ('sucre', 0.40907543897628784),\n",
       " ('blé', 0.4089013338088989),\n",
       " ('carburant', 0.40677961707115173),\n",
       " ('tabac', 0.40032240748405457),\n",
       " ('centimes', 0.3997615873813629),\n",
       " ('essence', 0.39639410376548767),\n",
       " ('palme', 0.3885374963283539),\n",
       " ('produits', 0.3879724442958832),\n",
       " ('passnavigo', 0.3873341381549835)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(\"prix\", topn=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.12892343"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similarity(w1=\"inflation\", w2=\"prix\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_vec(text: List, vocab: List, model: Word2Vec) -> np.ndarray: \n",
    "    \"\"\"Description. Convert text into embedding using word2vec.\"\"\"\n",
    "\n",
    "    return model.wv[np.array(text)[np.in1d(text, vocab)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = list(corpus)[:10]\n",
    "\n",
    "embeddings = [text_to_vec(sentence, vocab, model) for sentence in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet: le tout vendu des prix défiant toute concurrence ceci pour nous annoncer prochainement que ayant un trou dans notre économie circulaire nous allons participer au renflouement\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Embedding=[[ 0.85389745  2.358581    0.53904206 ...  0.9196409   1.3401673\n",
      "  -0.4548108 ]\n",
      " [-0.75303626 -0.09547216 -0.63800025 ...  0.68792623  1.9537251\n",
      "  -3.3618095 ]\n",
      " [-0.08319736 -0.5879957  -0.02148688 ... -0.0793516  -0.32891768\n",
      "   0.11505453]\n",
      " ...\n",
      " [ 0.38658124  0.02952377  0.44992316 ...  0.58287007  0.21661347\n",
      "   0.12841561]\n",
      " [ 0.05996609 -0.1715854  -0.7128676  ...  0.36932278  0.27788818\n",
      "  -0.669887  ]\n",
      " [ 1.3771836   1.6620768  -0.69492906 ...  1.5061085   0.98723197\n",
      "   0.4262609 ]]\n"
     ]
    }
   ],
   "source": [
    "for s, em in zip(sentences, embeddings): \n",
    "    break \n",
    "\n",
    "print(f\"Tweet: {' '.join(s)}\")\n",
    "print(\"-\"*100)\n",
    "print(f\"Embedding={em}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9 (tags/v3.9.9:ccb0e6a, Nov 15 2021, 18:08:50) [MSC v.1929 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4f0c75c3c98d859c17a8a7b8ea0d158e580de4be82cc7ea678b044820d2772f7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
